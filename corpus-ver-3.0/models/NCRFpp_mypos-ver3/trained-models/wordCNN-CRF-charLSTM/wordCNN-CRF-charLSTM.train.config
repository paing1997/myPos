### use # to comment out the configure item

###This model was trained using a LSTM structure to encode character sequences, CNN for word sequence information and CRF as inference layer.
###Learning rate 0.005 was used for word CNN models

### I/O ###
train_dir=/home/ye/tool/NCRFpp/mypos_data/train.bmes
dev_dir=/home/ye/tool/NCRFpp/mypos_data/dev.bmes
test_dir=/home/ye/tool/NCRFpp/mypos_data/test.bmes
model_dir=/home/ye/tool/NCRFpp/trained-models/wordCNN-CRF-charLSTM/ncrfpp
#word_emb_dir=mypos3/sample.word.bmes

#raw_dir=
#decode_dir=
#dset_dir=
#load_model_dir=
#char_emb_dir=

norm_word_emb=False
norm_char_emb=False
number_normalized=True
seg=True
word_emb_dim=50
char_emb_dim=30

###NetworkConfiguration###
use_crf=True
use_char=True
word_seq_feature=CNN
char_seq_feature=LSTM
#feature=[POS] emb_size=20
#feature=[Cap] emb_size=20
#nbest=1

###TrainingSetting###
status=train
optimizer=SGD
#iteration=100 #occur (ERROR: LOSS EXPLOSION (>1e8) ! PLEASE SET PROPER PARAMETERS AND STRUCTURE! EXIT....)
#iteration=1
iteration=10
batch_size=10
ave_batch_loss=False

###Hyperparameters###
cnn_layer=4
char_hidden_dim=50
hidden_dim=200
dropout=0.5
lstm_layer=1
bilstm=True

#learning_rate=0.015 #(iteration=10, not ok )
#learning_rate=0.005 #(iteration=10, ok , acc =0.8935932047145585)
#learning_rate=0.0005 #(iteration=10, ok)
#learning_rate=0.001 #(iteration=10, ok, accuracy= 0.9330128838260793)
#learning_rate=0.003 #(iteration=10,ok acc= 0.943789563155658)

learning_rate=0.004 #(iteration=10, ok, acc =0.9494523825201379)

#learning_rate=0.0049 #(iteration=10, ok , acc = 0.9461)
#learning_rate=0.0045 # (iteration=10, ok, acc = 0.9492328965562652,0.9399)

lr_decay=0.05
momentum=0
l2=1e-8
gpu=True
#clip=
